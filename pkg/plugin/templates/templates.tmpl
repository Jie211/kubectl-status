{{- define "DefaultResource" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- template "replicas_status" . }}
    {{- template "suspended" . }}
    {{- template "conditions_summary" .status.conditions }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end }}

{{- define "pod_status_summary_line" }}
    {{- template "status_summary_line" . }}
    {{- with .status.qosClass }} {{ if $.inline }}{{ . }}{{ else }}{{ . | colorKeyword }}{{ end }}{{ end }}
    {{- with .status.message }}, message: {{ . }}{{ end }}
{{- end -}}

{{- define "Pod" }}
    {{- template "pod_status_summary_line" . }}
    {{- template "application_details" . }}
    {{- include "pod_conditions_summary" .status.conditions | nindent 2 }}
    {{- if not .metadata.ownerReferences }}
        {{- $container := index .spec.containers 0 }}{{/* not ideal but will likely work in most cases if not all */}}
  {{ "Standalone POD" | red | bold }}{{ if $container.stdin }}, interactive{{ end }}{{ if $container.tty }} with attached TTY{{ end }}.
    {{- end }}
    {{- with .status.initContainerStatuses }}
  InitContainers:
        {{- range . }}
            {{- include "container_status_summary" . | nindent 4 }}
        {{- end }}
    {{- end }}
    {{- with .status.containerStatuses }}
  Containers:
        {{- $podMetrics := kubeGetFirst $.metadata.namespace "PodMetrics" $.metadata.name | default dict }}
        {{- range $containerStatus := . }}
            {{- if $podMetrics.containers }}
                {{- /* inject container metrics */ -}}
                {{- $containerMetrics := getItemInList $podMetrics.containers "name" .name }}
                {{- $containerStatus := set $containerStatus "metrics" $containerMetrics }}
                {{- /* inject container spec  */ -}}
                {{- $containerSpec := getItemInList $.spec.containers "name" .name }}
                {{- $containerStatus := set $containerStatus "spec" $containerSpec }}
            {{- end }}
            {{- include "container_status_summary" $containerStatus | nindent 4 }}
        {{- end }}
    {{- end }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
    {{- $pvcsHeader := true }}
    {{- range .spec.volumes }}
        {{- with .persistentVolumeClaim }}
            {{- if $pvcsHeader }}{{- $pvcsHeader = false }}
  PVCs:
            {{- end }}
            {{- $pvc := kubeGetFirst $.metadata.namespace "PersistentVolumeClaim" .claimName}}
            {{- with $pvc }}
                {{- include "PersistentVolumeClaim" . | nindent 4 }}
            {{- end }}
        {{- end }}
    {{- end }}
    {{- $svcs := kubeGetServicesMatchingPod . }}
    {{- $svcsHeader := true }}
    {{- range $svcs }}
        {{- if $svcsHeader }}{{- $svcsHeader = false }}
  Services matching this pod:
        {{- end }}
        {{- include "Service" . | nindent 4 }}
    {{- end}}
{{- end }}

{{- define "StatefulSet" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- $currentReplicas := .status.replicas | default 0.0 }}
    {{- $updatedReplicas := .status.updatedReplicas | default 0.0 }}
    {{- $readyReplicas := .status.readyReplicas | default 0.0 }}
    {{- /* Where there is no readyReplicas, STS doesn't have that fields at all,
           and apparantly the numbers are parsed as float 64, so used 0.0 rather then 0 */ -}}
    {{- $injectedStatus := .status }}
    {{- $injectedStatus = set $injectedStatus "readyReplicas" $readyReplicas }}
    {{- $injectedStatus = set $injectedStatus "currentReplicas" $currentReplicas }}
    {{- $injectedManifest := set . "status" $injectedStatus }}
    {{- template "replicas_status" $injectedManifest }}
    {{- template "suspended" . }}
    {{- $ongoingRollout := false }}
    {{- if and .status.currentRevision .status.updateRevision }}
        {{- $ongoingRollout = ne .status.currentRevision .status.updateRevision }}
    {{- end }}
    {{- template "conditions_summary" .status.conditions }}
    {{- if or (not $currentReplicas) (not $readyReplicas) }}
  {{ "Outage" | red | bold }}: Deployment has no Ready replicas.
        {{- if and .status.currentRevision (eq (.status.observedGeneration | default 1.0) 1.0) }}
  {{ "Stuck Initial Rollout?" | yellow | bold }} First rollout not yet progressed.
        {{- end }}
    {{- else if ne $currentReplicas $readyReplicas }}
        {{- if not $ongoingRollout }}
  {{ "Not Ready Replicas" | yellow | bold }}: {{ sub $currentReplicas $readyReplicas }} replicas are not Ready.
        {{- end }}
    {{- end }}
    {{- if $ongoingRollout }}
  {{ "Ongoing rollout" | yellow | bold }}: Update in progress ControllerRevision/{{ .status.currentRevision }} -> ControllerRevision/{{ .status.updateRevision }}, {{ $updatedReplicas }}/{{ .spec.replicas }} ({{ percent $updatedReplicas .spec.replicas | printf "%.0f"}}%).
        {{- if eq (.status.updatedCount | default 0.0) 1.0 }}
  {{ "Sruck Rollout?" | yellow | bold }}: Still replacing the first Pod, may indicate a stuck rollout.
        {{- end }}
        {{- with .diff }}
  {{ "Diff" | bold }}:
        {{- . | markRed "^-.*" | markGreen "^\\+.*" | nindent 4 }}
        {{- /*
        diff -u <(kubectl get --export -o json controllerrevisions/twxha01-thingworx-d46d4c69b | jq .data) <(kubectl get --export -o json controllerrevisions/twxha01-thingworx-7bc5578549 | jq .data)
        This can be used to show the diff between two: https://github.com/yudai/gojsondiff
        */ -}}
        {{- end }}
    {{- else }}
        {{- if and .spec.replicas (eq (float64 $injectedStatus.readyReplicas) 0.0) }}
  {{ "Stuck rollout" | red | bold }}: No ready replicas, this StatefulSet won't likely go further.
        {{- end }}
    {{- end }}
    {{- template "recent_statefulset_rollouts" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end }}

{{- define "recent_statefulset_rollouts" }}
    {{- $sectionHeader := false }}
    {{- range kubeGet .metadata.namespace "controllerrevisions" | default list }}
        {{- if eq (index .metadata.ownerReferences 0).name $.metadata.name }}
            {{- if not $sectionHeader }}
  Rollouts:
                {{- $sectionHeader = true }}
            {{- end }}
    {{ with .metadata.creationTimestamp }}{{ . | colorAgo }} ago{{ end }} managed by {{ .kind | bold }}/{{ .metadata.name }}
        {{- end }}
    {{- end }}
{{- end }}

{{- define "DaemonSet" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- template "daemonset_replicas_status" . }}
    {{- template "conditions_summary" .status.conditions }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "ReplicaSet" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- /* Where there is no readyReplicas, STS doesn't have that fields at all,
           and apparantly the numbers are parsed as float 64, so used 0.0 rather then 0 */ -}}
    {{- $injectedStatus := .status }}
    {{- if not (hasKey .status "readyReplicas") }}
        {{- $injectedStatus := set $injectedStatus "readyReplicas" 0.0 }}
    {{- end }}
    {{- if not (hasKey .status "availableReplicas") }}
        {{- $injectedStatus := set $injectedStatus "availableReplicas" 0.0 }}
    {{- end }}
    {{- $injectedManifest := set . "status" $injectedStatus }}
    {{- template "replicas_status" $injectedManifest }}
    {{- template "conditions_summary" .status.conditions }}
    {{- if and .spec.replicas (or (not .status.replicas) (not .status.readyReplicas)) }}
        {{- "Outage" | red | bold | nindent 2 }}: ReplicaSet has no Ready replicas.
    {{- end }}
    {{- if hasKey .metadata.annotations "deployment.kubernetes.io/desired-replicas" }}
        {{- $deploymentDesiredReplicas := index .metadata.annotations "deployment.kubernetes.io/desired-replicas" | float64 }}
        {{- if $deploymentDesiredReplicas }}
            {{- if .spec.replicas }}
                {{- if ne $deploymentDesiredReplicas (.spec.replicas | float64) }}
                    {{- "Ongoing rollout" | yellow | bold | nindent 2 }}, check Owner Reference resources.
                {{- end }}
            {{- else }}{{/* means .spec.replicas: 0 */}}
                {{- "Old" | red | bold | nindent 2 }}: This ReplicaSet is likely replaced by a new one, check Owner Reference resources.
            {{- end }}
        {{- else }}{{/* means deployment.kubernetes.io/desired-replicas: "0" */}}
            {{- template "suspended" . }}
        {{- end }}
    {{- end }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end }}

{{- define "Deployment" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- $injectedStatus := .status }}
    {{- if not (hasKey .status "replicas") }}
        {{- $injectedStatus := set $injectedStatus "replicas" 0.0 }}
    {{- end }}
    {{- if not (hasKey .status "readyReplicas") }}
        {{- $injectedStatus := set $injectedStatus "readyReplicas" 0.0 }}
    {{- end }}
    {{- if not (hasKey .status "availableReplicas") }}
        {{- $injectedStatus := set $injectedStatus "availableReplicas" 0.0 }}
    {{- end }}
    {{- $injectedManifest := set . "status" $injectedStatus }}
    {{- template "replicas_status" $injectedManifest }}
    {{- template "conditions_summary" .status.conditions }}
    {{- template "suspended" . }}
    {{- $currentReplicas := .status.replicas | default 0 | float64 }}
    {{- $updatedReplicas := .status.updatedReplicas | default 0 | float64 }}
    {{- $readyReplicas := .status.readyReplicas | default 0 | float64 }}
    {{- $ongoingRollout := and $currentReplicas (not (eq $currentReplicas $updatedReplicas)) }}
    {{- if $ongoingRollout }}
        {{- "Ongoing Rollout" | yellow | bold | nindent 2 }}: Update progress {{ $updatedReplicas }}/{{ .spec.replicas }} ({{ percent $updatedReplicas (float64 .spec.replicas) | printf "%.0f"}}%).
    {{- end }}
    {{- if or (not $currentReplicas) (not $readyReplicas) }}
        {{- "Outage" | red | bold | nindent 2 }}: Deployment has no Ready replicas.
        {{- if eq (.status.observedGeneration | default 1.0 | float64) 1.0 }}
            {{- "Stuck Initial Rollout?" | yellow | bold | nindent 2 }} First rollout not yet progressed.
        {{- end }}
    {{- else if ne $currentReplicas $readyReplicas }}
        {{- if not $ongoingRollout }}
            {{- "Not Ready Replicas" | yellow | bold | nindent 2 }}: {{ sub $currentReplicas $readyReplicas }} replicas are not Ready.
        {{- end }}
    {{- else if .status.unavailableReplicas }}
        {{- if not $ongoingRollout }}
            {{- "Unavailable Replicas" | yellow | bold | nindent 2 }}: {{ .status.unavailableReplicas }} replicas are not Available.
        {{- end }}
    {{- end }}
    {{- template "recent_deployment_rollouts" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end }}

{{- define "recent_deployment_rollouts" }}
    {{- $sectionHeader := false }}
    {{- range kubeGet .metadata.namespace "ReplicaSets" | default list }}
        {{- if eq (index .metadata.ownerReferences 0).name $.metadata.name }}
            {{- if not $sectionHeader }}
  Rollouts:
                {{- $sectionHeader = true }}
            {{- end }}
    {{ with .metadata.creationTimestamp }}{{ . | colorAgo }} ago{{ end }} managed by {{ .kind | bold }}/{{ .metadata.name }}
        {{- end }}
    {{- end }}
{{- end }}

{{- define "suspended" }}
    {{- if hasKey .spec "replicas" }}
        {{- if not .spec.replicas }}
            {{- "Suspended" | red | bold | nindent 2 }}: Scaled down to 0.
        {{- end }}
    {{- end }}
{{- end }}

{{- define "recent_updates" }}
    {{- with .metadata.managedFields }}
  Known/recorded manage events:
        {{- range . }}
    {{ .time | colorAgo }} ago {{ .operation | bold }}d by {{ .manager | bold }} ({{ .fieldsV1 | default dict | keys | sortAlpha | join ", " | replace "f:" "" }})
        {{- end }}
    {{- end }}
{{- end }}

{{- define "PersistentVolume" }}
    {{- template "status_summary_line" . }}
  PV is {{- with .status.phase }} {{ . | colorKeyword }}{{ end }}
    {{- with .spec.storageClassName  }} managed by {{ "StorageClass" | bold }}/{{ . }}{{ end }}
    {{- with index .metadata.annotations "kubernetes.io/createdby" }} created by {{ . | bold }}{{ end }}
    {{- with index .metadata.annotations "pv.kubernetes.io/provisioned-by" }} provisioned by {{ . | bold }}{{ end }}
    {{- with index .spec.accessModes 0 }} with {{ . | bold }} mode{{ end }}
    {{- with .status.reason }}{{ "reason" | bold }}: {{ . }}{{ end }}
    {{- with .status.message }}{{ "message" | red | bold | nindent 2 }}: {{ . }}{{- end }}{{/* Exists usually when there is problem */}}
    {{- with .spec.claimRef }}
  Created for {{ .kind | bold }}/{{ .name }} -n {{ .namespace }}
        {{- $pvc := kubeGetFirst .namespace .kind .name }}
        {{- if $pvc }}
            {{- if ne $pvc.metadata.uid .uid }}
    {{ "Dangling" | red | bold }}: The PVC referenced in this PV is replaced by a new one. And a new PV is created for the replacement PVC.
      PVC uid referenced in this PV: {{ .uid }}
      Current PVC uid: {{ $pvc.metadata.uid }}
            {{- end }}
        {{- else }}
    {{ "Replaced" | red | bold }}: This PVC doesn't exist anymore. Its likely that this PV is dangling.
        {{- end }}
    {{- end }}
    {{- with .spec.azureDisk }}
  {{ "Azure Disk" | bold }}
        {{- with .kind }} of kind {{ . | bold }}{{ end }}
        {{- with .readOnly }}, in {{ "RO" | bold | yellow }} mode{{ end }}
        {{- with .cachingMode }}, with {{ . | bold }} host cache{{end}}
        {{- with .fsType }}, using {{ . | bold }} FS type{{end}}
    Disk URI: {{ .diskURI }}
    {{- end }}
    {{- with .spec.azureFiles }}
  {{ "Azure Files" | bold }}
        {{- with .shareName }}, share name is {{ . | bold }}{{end}}
        {{- with .readOnly }}, in {{ "RO" | bold | yellow }} mode{{ end }}
    {{- end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "PersistentVolumeClaim" }}
    {{- template "status_summary_line" . }}
    {{- template "application_details" . }}
    {{- template "conditions_summary" .conditions }}
  {{ "PVC" }}
    {{- with .spec.volumeName }} uses {{ "PersistentVolume" | bold }}/{{ . }}{{ end }}
    {{- with .spec.volumeMode }}, with {{ . | bold }} mode{{ end }}
    {{- with .status.capacity.storage }}, asks for {{ . | bold }}{{ end }}
    {{- with .metadata.annotations }}
        {{- with index . "volume.beta.kubernetes.io/storage-provisioner" }}, provisioned by {{ . | bold }}{{ end }}
        {{- with index . "volume.kubernetes.io/selected-node" }}, attached on {{ "Node" | bold }}/{{ . }}{{ end }}
    {{- end }}
    {{- if not .spec.volumeName }}
  {{ "Pending" | red | bold }}: This PVC doesnt yet have a paired PV.
    {{- end }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
    {{- with .spec.volumeName }}
  Binds:
        {{- include "PersistentVolume" (kubeGetFirst "" "PersistentVolume" .) | nindent 4 }}
    {{- end }}
{{- end -}}

{{- define "ComponentStatus" }}
    {{- template "status_summary_line" . }}
    {{- template "application_details" . }}
    {{- template "conditions_summary" .conditions }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "CronJob" }}
    {{- template "status_summary_line" . }}
    {{- if .status.lastScheduleTime }}, last ran at {{ .status.lastScheduleTime }} ({{ .status.lastScheduleTime | colorAgo }} ago)
    {{- else }}
  {{ "Not yet scheduled" | yellow | bold }}
    {{- end }}
    {{- with .status.active }}
        {{- range . }}
  {{ "Active" | green }}: {{ .kind | bold }}/{{ .name }} is running.
        {{- end }}
    {{- end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "Job" }}
    {{- template "status_summary_line" . }}
    {{- /* See https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#parallel-jobs */ -}}
    {{- if eq (coalesce .spec.completions .spec.parallelism 1 | toString) "1" }}
        {{- template "job_non_parallel" . }}
    {{- else if .spec.completions }}
        {{- /* TODO: handle "fixed completion count jobs" better */ -}}
        {{- template "job_parallel" . }}
    {{- else if .spec.parallelism }}
        {{- /* TODO: handle "work queue jobs" better */ -}}
        {{- template "job_parallel" . }}
    {{- end }}
    {{- template "conditions_summary" .status.conditions }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "job_non_parallel" }}
    {{- if .status.active }}, {{ "Active" | green }}{{ end }}
    {{- if .status.succeeded }}, {{ "Succeeded" | green }}{{ end }}
    {{- if .status.failed }}, {{ "Failed" | red | bold }}{{ end }}
{{- end -}}

{{- define "job_parallel" }}
    TODO: handle parallel jobs  better
    {{- if .status.active }}, {{ "active" | green }}:{{ . }}{{ end }}
    {{- if .status.failed }}, {{ "failed" | red | bold }} {{ .status.failed }}/{{ .spec.backoffLimit }} times{{ end }}
{{- end -}}

{{- define "Service" }}
    {{- template "status_summary_line" . }}
    {{- if eq .spec.clusterIP "None" }} {{ "Headless" | yellow | bold }}{{- end }}
    {{- if eq .spec.type "LoadBalancer" }}
        {{- template "load_balancer_ingress" . }}
    {{- end }}
    {{- $endpoint := kubeGetFirst .metadata.namespace "Endpoints" .metadata.name }}
    {{- if $endpoint }}
        {{- if hasKey $endpoint.metadata.annotations "endpoints.kubernetes.io/last-change-trigger-time" -}}
            , last endpoint change was {{ index $endpoint.metadata.annotations "endpoints.kubernetes.io/last-change-trigger-time"  | colorAgo }} ago
        {{- end}}
        {{- if $endpoint.subsets }}
            {{- range $endpoint.subsets }}
                {{- $ports := .ports }}
                {{- if .addresses }}
                    {{- range .addresses }}
  Ready: {{ template "endpoint_subnet_address" (dict "address" . "ports" $ports) }}
                    {{- end }}
                {{- else }}
  {{ "Outage" | red | bold }}: This service doesn't match any Ready pods.
                {{- end }}
                {{- with .notReadyAddresses }}
                    {{- range . }}
  {{ "NotReady" | red | bold }}: {{ template "endpoint_subnet_address" (dict "address" . "ports" $ports) }}
                    {{- end }}
                {{- end }}
            {{- end }}
        {{- else }}
  {{ "No matching pods" | red | bold }}: Service selector either doens't match any Pods or the Service's targetPort doesn't match the Pod's port.
        {{- end }}
    {{- else }}
  {{ "Missing Endpoint" | red | bold }}: Service has no matching endpoint.
    {{- end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "Ingress" }}
    {{- template "status_summary_line" . }}
    {{- template "load_balancer_ingress" . }}
    {{- with .unsupportedApiVersion }}
  {{ "Unsupported Api version" | yellow }}: apiVersion {{ . }} is not supported for detailed backend issues!
    {{- end }}
    {{- with .backendIssues }}
        {{- range . }}
            {{- if eq .IssueType "serviceMissing" }}
  {{ "Service doesn't exist" | red | bold }}: {{ "Service" | bold }}/{{ .Backend.ServiceName }}:{{ .Backend.ServicePort }} referenced in ingress, but it doesn't exist.
            {{- else if eq .IssueType "serviceWithPortMismatch" }}
  {{ "Service Port doesn't exist" | red | bold }}: {{ "Service" | bold }}/{{ .Backend.ServiceName }}:{{ .Backend.ServicePort }} referenced in ingress, but Service doesnt have that port defined.
            {{- else if eq .IssueType "serviceWithNoReadyAddresses" }}
  {{ "Service outage" | red | bold }}: {{ "Service" | bold }}/{{ .Backend.ServiceName }}:{{ .Backend.ServicePort }} referenced in ingress doesn't have any Healthy endpoints.
            {{- end }}
        {{- end }}
    {{- end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "HorizontalPodAutoscaler" }}
    {{- template "status_summary_line" . }} last scale was {{ .status.lastScaleTime | colorAgo }} ago
  {{ "current" | bold }} replicas:{{ .status.currentReplicas }}/({{ .spec.minReplicas | default "1" }}-{{ .spec.maxReplicas }})
    {{- if .status.currentCPUUtilizationPercentage }} CPUUtilisation: {{ .status.currentCPUUtilizationPercentage | toString | redIf (ge .status.currentCPUUtilizationPercentage .spec.targetCPUUtilizationPercentage) }}%/{{ .spec.targetCPUUtilizationPercentage }}%{{ end }}
    {{- if (ne .status.currentReplicas .status.desiredReplicas) }}, {{ "desired" | red | bold}}: {{ .status.currentReplicas }} --> {{ .status.desiredReplicas }}{{ end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "ResourceQuota" }}
    {{- template "status_summary_line" . }}
    {{- $hard := .status.hard }}
    {{- range $key, $value := .status.used }}
  {{ $key }}: {{ $value }}/{{ index $hard $key }}
    {{- end }}
    {{- template "application_details" . }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "load_balancer_ingress" }}
    {{- if .status.loadBalancer.ingress }}
        {{- if or (index .status.loadBalancer.ingress 0).hostname (index .status.loadBalancer.ingress 0).ip }}
            {{- with (index .status.loadBalancer.ingress 0).hostname }} {{ "LoadBalancer" | green }}:{{ . }}{{ end }}
            {{- with (index .status.loadBalancer.ingress 0).ip }} {{ "LoadBalancer" | green }}:{{ . }}{{ end }}
        {{- else }} {{ "Pending LoadBalancer" | red | bold }}
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "endpoint_subnet_address" }}
    {{- $ip := .address.ip }}
    {{- $hasTargetRef := not (not .address.targetRef) }}
    {{- if .address.targetRef }}
        {{- .address.targetRef.kind | bold }}/{{ .address.targetRef.name }}
        {{- with .address.targetRef.namespace }} -n {{ . }}{{ end }}
        {{- with .address.nodeName }} on {{ "Node" | bold }}/{{ . }}{{ end }}
    {{- end }}
    {{- range $index, $port := .ports }}
        {{- if $hasTargetRef }}, {{ else }}{{if $index}}, {{end}}{{ end }}
        {{- $ip }}:{{ $port.port }}/{{ $port.protocol }}{{ with $port.name }} ({{ . }}){{ end }}
    {{- end }}
{{- end -}}

{{- define "daemonset_replicas_status" }}
    {{- if .status.desiredNumberScheduled }}{{ $desiredNumberScheduled := float64 .status.desiredNumberScheduled }}
  {{ printf "desired:%.0f" (float64 .status.desiredNumberScheduled) | bold }}
        {{- if hasKey .status "currentNumberScheduled" }}, {{ printf "current:%.0f" (float64 .status.currentNumberScheduled) | toString | redBoldIf (not ( eq $desiredNumberScheduled (float64 .status.currentNumberScheduled) )) }}{{ end }}
        {{- if hasKey .status "numberAvailable" }}, {{ printf "available:%.0f" (float64 .status.numberAvailable) | toString | redBoldIf (not ( eq $desiredNumberScheduled (float64 .status.numberAvailable) )) }}{{ end }}
        {{- if hasKey .status "numberReady" }}, {{ printf "ready:%.0f" (float64 .status.numberReady) | toString | redBoldIf (not ( eq $desiredNumberScheduled (float64 .status.numberReady) )) }}{{ end }}
        {{- if hasKey .status "updatedNumberScheduled" }}, {{ printf "updated:%.0f" (float64 .status.updatedNumberScheduled) | toString | redBoldIf (not ( eq $desiredNumberScheduled (float64 .status.updatedNumberScheduled) )) }}{{ end }}
        {{- with .status.numberMisscheduled }}{{ "numberMisscheduled" | red | bold }}:{{ . }}{{- end }}
    {{- end }}
{{- end -}}

{{- define "replicas_status" }}
    {{- if hasKey .status "replicas" }}{{ $spec_replicas := float64 .spec.replicas }}
  {{ printf "desired:%.0f" (float64 .spec.replicas) | bold }}
        {{- with .status.replicas }}, {{ printf "existing:%.0f" (float64 .) | toString | redBoldIf (not ( eq $spec_replicas (float64 .) )) }}{{ end }}
        {{- if hasKey .status "readyReplicas" }}, {{ printf "ready:%.0f" (float64 .status.readyReplicas) | toString | redBoldIf (not ( eq $spec_replicas (float64 .status.readyReplicas) )) }}{{ end }}
        {{- if hasKey .status "currentReplicas" }}, {{ printf "current:%.0f" (float64 .status.currentReplicas) | toString | redBoldIf (not ( eq $spec_replicas (float64 .status.currentReplicas) )) }}{{ end }}
        {{- if hasKey .status "updatedReplicas" }}, {{ printf "updated:%.0f" (float64 .status.updatedReplicas) | toString | redBoldIf (not ( eq $spec_replicas (float64 .status.updatedReplicas) )) }}{{ end }}
        {{- if hasKey .status "availableReplicas" }}, {{ printf "available:%.0f" (float64 .status.availableReplicas) | toString | redBoldIf (not ( eq $spec_replicas (float64 .status.availableReplicas) )) }}{{ end }}
        {{- if hasKey .status "fullyLabeledReplicas" }}, {{ printf "fullyLabeled:%.0f" (float64 .status.fullyLabeledReplicas) | toString | redBoldIf (not ( eq $spec_replicas (float64 .status.fullyLabeledReplicas) )) }}{{ end }}
        {{- with .status.unavailableReplicas }}, {{ printf "unavailable:%.0f" (float64 .) | toString | red | bold }}{{ end }}
        {{- with .status.collisionCount }}, {{ printf "collisions:%.0f" (float64 .) | toString | red | bold }}{{ end }}
    {{- end }}
{{- end -}}

{{- define "status_summary_line" }}
    {{- if .inline }}
        {{- .kind | bold }}/{{ .metadata.name }}
    {{- else}}
        {{- .kind | cyan | bold }}/{{ .metadata.name | cyan }}
    {{- end}}
    {{- with .metadata.namespace }} -n {{ . }}{{ end }}
    {{- with .metadata.creationTimestamp }}, created {{ . | colorAgo }} ago{{ end }}
    {{- if .metadata.ownerReferences }} by {{ range $index, $ownerReference := .metadata.ownerReferences }}
        {{- if $index }},{{ end }}{{ $ownerReference.kind | bold }}/{{ $ownerReference.name }}
    {{- end }}{{ end }}
    {{- with .metadata.generation }}, gen:{{ . }}{{ end }}
    {{- if .status.startTime }}
        {{- $created := .metadata.creationTimestamp | toDate "2006-01-02T15:04:05Z" }}
        {{- $started := .status.startTime | toDate "2006-01-02T15:04:05Z" }}
        {{- $startedIn := $started.Sub $created}}
        {{- if gt ($startedIn.Seconds | int) 0 }}, started after {{ $startedIn.Seconds | ago }}{{ end }}
    {{- end }}
    {{- if .status.completionTime }}
        {{- $started := .status.startTime | toDate "2006-01-02T15:04:05Z" -}}
        {{- $completed := .status.completionTime | toDate "2006-01-02T15:04:05Z" -}}
        {{- $ranfor := $completed.Sub $started }} and {{ "completed" | green }} in {{ $ranfor | colorDuration }}
    {{- end }}
    {{- with .status.phase }} {{ if $.nodeContext }}{{ . | redIf (eq . "Pending" ) }}{{else}}{{ . | colorKeyword }}{{ end }}{{ end }}
    {{- /* .status.state is used by e.g. Ambassador */ -}}
    {{- with .status.state }} {{ . | colorKeyword }}{{ end }}
    {{- with .status.reason }} {{ . | colorKeyword }}{{ end }}
    {{- if .metadata.deletionTimestamp }} {{ "DELETING" | red | bold }} {{- end }}
{{- end -}}

{{- define "observed_generation_summary" }}
    {{- if and .metadata.generation .status.observedGeneration }}
        {{- if ne .metadata.generation .status.observedGeneration }}
observedGeneration({{ .status.observedGeneration | red | bold }}) doesn't match generation({{ .metadata.generation | red | bold }})
  {{ "This usually means related controller has not yet reconciled this resource!" | yellow }}
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "pod_conditions_summary" }}
    {{- $podScheduledCondition := getItemInList . "type" "PodScheduled" }}
    {{- $readyCondition := getItemInList . "type" "Ready" }}
    {{- if (isStatusConditionHealthy $podScheduledCondition) }}
        {{- "PodScheduled" | bold }}
    {{- else }}
        {{- "Not PodScheduled" | red | bold }}
    {{- end }}
    {{- " -> "}}
    {{- $initializedCondition := getItemInList . "type" "Initialized" }}
    {{- if (isStatusConditionHealthy $initializedCondition) }}
        {{- "Initialized" | bold }}
    {{- else }}
        {{- "Not Initialized" | red | bold }}
    {{- end }}
    {{- " -> "}}
    {{- $containersReadyCondition := getItemInList . "type" "ContainersReady" }}
    {{- if (isStatusConditionHealthy $containersReadyCondition) }}
        {{- "ContainersReady" | bold }}
    {{- else }}
        {{- "Not ContainersReady" | red | bold }}
    {{- end }}
    {{- " -> "}}
    {{- if (isStatusConditionHealthy $readyCondition) }}
        {{- template "condition_summary" $readyCondition }}
    {{- else }}
        {{- "Not Ready" | red | bold }}
    {{- end }}
    {{- range . }}
        {{- /* show details for only unhealthy conditions */ -}}
        {{- if (not (isStatusConditionHealthy .)) }}
            {{- include "condition_summary" . | nindent 2}}
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "conditions_summary" }}
    {{- if . }}
        {{- range . }}
  {{ template "condition_summary" . }}
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "condition_summary" }}
    {{- $condition := . }}
    {{- .type | redIf (not (isStatusConditionHealthy .)) | bold }}
    {{- with .reason }} {{ . | redBoldIf (not (isStatusConditionHealthy $condition)) }}{{ end }}
    {{- with .message }}, {{ . | redIf (not (isStatusConditionHealthy $condition)) }}{{ end }}
    {{- with .lastTransitionTime }} for {{ . | colorAgo }}{{ end }}
    {{- if .lastUpdateTime }}
        {{- if ne (.lastUpdateTime | colorAgo) (.lastTransitionTime | colorAgo) -}}
            , last update was {{ .lastUpdateTime | colorAgo }} ago
        {{- end }}
    {{- end }}
    {{- if .lastProbeTime}}
        {{- if ne (.lastProbeTime | colorAgo) (.lastTransitionTime | colorAgo) -}}
            , last probe was {{ .lastProbeTime | colorAgo }} ago
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "container_usage" -}}
    cpu usage:{{ .metrics.usage.cpu | quantityToFloat64 | printf "%.1g" -}}/[
    {{- if .spec.resources.requests.cpu -}}
        {{- percent (.metrics.usage.cpu | quantityToFloat64) (.spec.resources.requests.cpu | quantityToFloat64) | colorPercent "%.0f%% " -}}
        of req:{{ .spec.resources.requests.cpu | quantityToFloat64 | printf "%.1g" }}
    {{- else }}{{ "no-req" | yellow }}
    {{- end }}, {{ if .spec.resources.limits.cpu -}}
        {{- percent (.metrics.usage.cpu | quantityToFloat64) (.spec.resources.limits.cpu | quantityToFloat64) | colorPercent "%.0f%% " -}}
        of lim:{{ .spec.resources.limits.cpu | quantityToFloat64 | printf "%.1g" }}
    {{- else }}{{ "no-lim" | yellow }}
    {{- end -}}
    ], mem usage:{{- .metrics.usage.memory | quantityToFloat64 | humanizeSI "B" -}}/[
    {{- if .spec.resources.requests.memory -}}
        {{- percent (.metrics.usage.memory | quantityToFloat64) (.spec.resources.requests.memory | quantityToFloat64) | colorPercent "%.0f%% " -}}
        of req:{{ .spec.resources.requests.memory | quantityToFloat64 | humanizeSI "B" }}
    {{- else }}{{ "no-req" | yellow }}
    {{- end }}, {{ if .spec.resources.limits.memory -}}
        {{- percent (.metrics.usage.memory | quantityToFloat64) (.spec.resources.limits.memory | quantityToFloat64) | colorPercent "%.0f%% " -}}
        of lim:{{ .spec.resources.limits.memory | quantityToFloat64 | humanizeSI "B" }}
    {{- else }}{{ "no-lim" | yellow }}
    {{- end -}}
    ]
{{- end -}}

{{- define "container_status_summary" }}
    {{- .name | bold }} ({{ .image | markYellow "latest" }}) {{ template "container_state_summary" .state }}
    {{- if .state.running }}{{ if .ready }} and {{ "Ready" | green }}{{ else }} but {{ "Not Ready" | red | bold }}{{ end }}{{ end }}
    {{- if gt (.restartCount | int ) 0 }}, {{ printf "restarted %d times" (.restartCount | int) | yellow | bold }}{{ end }}
    {{- if .metrics }}
        {{- if .metrics.usage.cpu }}
  usage {{ template "container_usage" . }}
        {{- end }}
    {{- end }}
    {{- with .lastState }}
  previously: {{ template "container_state_summary" . }}
    {{- end }}
{{- end -}}

{{- define "container_state_summary" }}
    {{- /* https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#pod-and-container-status */}}
    {{- with .waiting }}
        {{- "Waiting" | red | bold }} {{ .reason | red | bold }}{{ with .message }}: {{ . | red | bold }}{{ end }}
    {{- end }}
    {{- with .running }}
        {{- "Running" | green }} for {{ .startedAt | colorAgo }}
    {{- end }}
    {{- with .terminated }}
        {{- if .startedAt }}
            {{- $started := .startedAt | toDate "2006-01-02T15:04:05Z" -}}
            {{- $finished := .finishedAt | toDate "2006-01-02T15:04:05Z" -}}
            {{- $ranfor := $finished.Sub $started -}}
        Started {{ .startedAt | colorAgo }} ago and {{ if .reason }}{{ .reason | colorKeyword }}{{ else }}terminated{{ end }} after {{ $ranfor | colorDuration }}
            {{- if .exitCode }} with exit code {{ template "exit_code_summary" . }}{{ end }}
        {{- else }}
            {{ template "exit_code_summary" . }}
        {{- end }}
    {{- end }}
{{- end -}}

{{- define "exit_code_summary" }}
    {{- .exitCode | toString | redIf (ne (.exitCode | toString) "0" ) }}
    {{- with .signal }} (signal: {{ . }}){{ end }}
    {{- if and (gt (.exitCode | int) 128) (le (.exitCode | int) 165) }} ({{ sub (.exitCode | int) 128 | signalName }}) {{ end }}
{{- end -}}

{{- define "events" }}
    {{- with getEvents . }}{{ if .items }}
  Events:
        {{- range .items }}
    {{ "" }} {{- /* To have the newline and two spaces */ -}}
            {{- if eq .type "Warning" }}{{ .reason | red | bold }} {{ else }}{{ .reason }} {{ end }}
            {{- with .lastTimestamp }}{{ . | colorAgo }} ago {{ end }}
            {{- if gt (.count | int) 1 -}}
                {{- if .firstTimestamp -}}
                    (x{{ .count }} over {{ .firstTimestamp | colorAgo }}) {{ "" }}
                {{- else -}}
                    (x{{ .count }}) {{ "" }}
                {{- end -}}
            {{- end -}}
            from {{ list .reportingComponent .reportingInstance .source.component .source.host .source.fieldPath | compact | join "," | bold }}: {{ .message }}
        {{- end }}
    {{- end }}{{- end }}
{{- end -}}

{{- define "owners" }}
    {{- if .metadata.ownerReferences }}
  Owners:{{ includeOwners . | indent 4 }}
    {{- end }}
{{- end }}

{{- define "NamespaceConfig" }}
    {{- template "status_summary_line" . }}
    {{- template "observed_generation_summary" . }}
    {{- template "application_details" . }}
    {{- template "conditions_summary" .status.conditions }}
    {{- range $resource, $valueList := .status.lockedResourceStatuses }}
  {{ $resource }}: {{ range $status := $valueList }}{{ template "condition_summary" . }}{{- end }}
    {{- end }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end }}

{{- define "Node" }}
    {{- /* also include info from `kubectl get --raw /api/v1/nodes/<node>/proxy/stats/summary` */ -}}
    {{- template "status_summary_line" . }}
    {{- template "application_details" . }}
  {{ .status.nodeInfo.operatingSystem | bold }} {{ .status.nodeInfo.osImage }} ({{ .status.nodeInfo.architecture }}), kernel {{ .status.nodeInfo.kernelVersion }}, kubelet {{ .status.nodeInfo.kubeletVersion }}, kube-proxy {{ .status.nodeInfo.kubeProxyVersion }}
    {{- if or (index .metadata.labels "node.kubernetes.io/instance") (index .metadata.labels "topology.kubernetes.io/region") (index .metadata.labels "failure-domain.beta.kubernetes.io/region") (index .metadata.labels "topology.kubernetes.io/zone") (index .metadata.labels "failure-domain.beta.kubernetes.io/region") }}
  {{ "cloudprovider" | bold }}
        {{- with index .metadata.labels "topology.kubernetes.io/region" | default (index .metadata.labels "failure-domain.beta.kubernetes.io/region") }} {{ . }}{{ end }}
        {{- with index .metadata.labels "topology.kubernetes.io/zone" | default (index .metadata.labels "failure-domain.beta.kubernetes.io/zone") }}{{ . }}{{ end }}
        {{- with index .metadata.labels "node.kubernetes.io/instance" | default (index .metadata.labels "beta.kubernetes.io/instance-type") }} {{ . }}{{ end }}
        {{- with .metadata.labels.agentpool }}, agentpool:{{ . }}{{ end }}
        {{- with index .metadata.labels "kubernetes.io/role" }}, role:{{ . }}{{ end }}
    {{- end }}
  {{ "images" | bold }} {{ .status.images | len }}
    {{- if .status.volumesInUse }} {{ "volumes" | bold }} inuse={{ .status.volumesInUse | len }}
        {{- with index .status.allocatable "attachable-volumes-azure-disk" }}/{{ . }}{{ end }}, attached={{ .status.volumesAttached | len }}
    {{- end}}
    {{- template "conditions_summary" .status.conditions }}
    {{- with kubeGetFirst "kube-node-lease" "Lease" .metadata.name }}
  {{ "Lease" | bold }} for {{ .spec.leaseDurationSeconds }}s, renew in {{ .spec.renewTime | ago }}
    {{- end }}
    {{- $podCount := 0 }}
    {{- $podsTotalCpuUsage := 0.0 }}
    {{- $podsTotalCpuRequests := 0.0 }}
    {{- $podsTotalCpuLimits := 0.0 }}
    {{- $podsTotalMemUsage := 0.0 }}
    {{- $podsTotalMemRequests := 0.0 }}
    {{- $podsTotalMemRequestsBurstable := 0.0 }}
    {{- $podsTotalMemRequestsGuaranteed := 0.0 }}
    {{- $podsTotalMemLimits := 0.0 }}
    {{- with .pods }}
        {{- range $pod := . }}
            {{- $podCount = $podCount | add 1 }}
            {{- with .spec.containers }}
                {{- range $containerSpec := . }}
                    {{- if $containerSpec.resources.requests.cpu }}
                        {{- $podsTotalCpuRequests = $podsTotalCpuRequests | addFloat64 ($containerSpec.resources.requests.cpu | quantityToFloat64) }}
                    {{- end }}
                    {{- if $containerSpec.resources.limits.cpu }}
                        {{- $podsTotalCpuLimits = $podsTotalCpuLimits | addFloat64 ($containerSpec.resources.limits.cpu | quantityToFloat64) }}
                    {{- end }}
                    {{- if $containerSpec.resources.requests.memory }}
                        {{- $podsTotalMemRequests = $podsTotalMemRequests | addFloat64 ($containerSpec.resources.requests.memory | quantityToFloat64) }}
                        {{- if eq $pod.status.qosClass "Burstable" }}
                            {{- $podsTotalMemRequestsBurstable = $podsTotalMemRequestsBurstable | addFloat64 ($containerSpec.resources.requests.memory | quantityToFloat64) }}
                        {{- else if eq $pod.status.qosClass "Guaranteed"  }}
                            {{- $podsTotalMemRequestsGuaranteed = $podsTotalMemRequestsGuaranteed | addFloat64 ($containerSpec.resources.requests.memory | quantityToFloat64) }}
                        {{- end }}
                    {{- end }}
                    {{- if $containerSpec.resources.limits.memory }}
                        {{- $podsTotalMemLimits = $podsTotalMemLimits | addFloat64 ($containerSpec.resources.limits.memory | quantityToFloat64) }}
                    {{- end }}
                    {{- if $pod.podMetrics.containers }}
                        {{- $containerMetrics := getItemInList $pod.podMetrics.containers "name" .name }}
                        {{- if $containerMetrics }}
                            {{- $podsTotalCpuUsage = $podsTotalCpuUsage | addFloat64 ($containerMetrics.usage.cpu | quantityToFloat64) }}
                            {{- $podsTotalMemUsage = $podsTotalMemUsage | addFloat64 ($containerMetrics.usage.memory | quantityToFloat64) }}
                        {{- end }}
                    {{- end }}
                {{- end }}
            {{- end }}
        {{- end }}
    {{- end }}
    {{- $inferredBurstableMemLimit := .status.allocatable.memory | quantityToFloat64 | subFloat64 $podsTotalMemRequestsGuaranteed }}
    {{- $inferredBestEffortMemLimit := $inferredBurstableMemLimit | subFloat64 $podsTotalMemRequestsBurstable }}
  Inferred memory limits for BestEffort: {{ $inferredBestEffortMemLimit | humanizeSI "B" }}, for Burstable: {{ $inferredBurstableMemLimit | humanizeSI "B" }}
    {{- $nodeMetrics := kubeGetFirst "" "NodeMetrics" .metadata.name | default dict }}
    {{- if $nodeMetrics.usage }}
  {{ "pods" | bold }}: usage/allocatable:{{ $podCount }}/
        {{- .status.allocatable.pods | quantityToFloat64 | printf "%g" }}(
        {{- percent ($podCount | float64) (.status.allocatable.pods | quantityToFloat64) | colorPercent "%.0f%%" }})
        {{- if eq ($podCount | float64) (.status.allocatable.pods | quantityToFloat64) }}
    {{ "Reached Max Pods Limit" | red | bold }}: Number of Pods that can run on this Kubelet (`--max-pods`) limit is reached. No more pods will be scheduled to this Node even if there is free resource (E.g. cpu/mem).
        {{- end }}
  {{ "cpu" | bold }}: usage/allocatable:{{ $nodeMetrics.usage.cpu | quantityToFloat64 | printf "%g" }}/
        {{- .status.allocatable.cpu | quantityToFloat64 | printf "%g" }}(
        {{- percent ($nodeMetrics.usage.cpu | quantityToFloat64) (.status.allocatable.cpu | quantityToFloat64) | colorPercent "%.0f%%" }})
        {{- $percent := percent $podsTotalCpuUsage (.status.allocatable.cpu | quantityToFloat64) }}
        {{- "" }}, allocated usage:{{ $podsTotalCpuUsage | printf "%.3g" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable)/[req:
        {{- $percent := percent $podsTotalCpuRequests (.status.allocatable.cpu | quantityToFloat64) }}
        {{- $podsTotalCpuRequests | printf "%.3g" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable), lim:
        {{- $percent := percent $podsTotalCpuLimits (.status.allocatable.cpu | quantityToFloat64) }}
        {{- $podsTotalCpuLimits | printf "%.3g" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable)]
        {{- if ge $podsTotalCpuLimits (.status.allocatable.cpu | quantityToFloat64) }}
    {{ "CPU overcommitted" | red | bold }}: Application may be slow, applications are more likely to race for free cpu cycles.
        {{- end }}
  {{ "mem" | bold }}: usage/allocatable:{{ $nodeMetrics.usage.memory | quantityToFloat64 | humanizeSI "B" }}/
        {{- .status.allocatable.memory | quantityToFloat64 | humanizeSI "B" }}(
        {{- percent ($nodeMetrics.usage.memory | quantityToFloat64) (.status.allocatable.memory | quantityToFloat64) | colorPercent "%.0f%%" }})
        {{- $percent := percent $podsTotalMemUsage (.status.allocatable.memory | quantityToFloat64) }}
        {{- "" }}, allocated usage:{{ $podsTotalMemUsage | humanizeSI "B" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable)/[req:
        {{- $percent := percent $podsTotalMemRequests (.status.allocatable.memory | quantityToFloat64) }}
        {{- $podsTotalMemRequests | humanizeSI "B" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable), lim:
        {{- $percent := percent $podsTotalMemLimits (.status.allocatable.memory | quantityToFloat64) }}
        {{- $podsTotalMemLimits | humanizeSI "B" }}({{ $percent | colorPercent "%.0f%%" }} of allocatable)]
        {{- if ge $podsTotalMemLimits (.status.allocatable.memory | quantityToFloat64) }}
    {{ "Memory overcommitted" | red | bold }}: Application may be OOMKilled. Expect BestEffort and then Burstable pods to have OOMKill due to overcommitted memory on the node.
        {{- end }}
  {{ "ephemeral-storage" | bold }}: {{index .status.allocatable "ephemeral-storage" | quantityToFloat64 | humanizeSI "B" }}
    {{- else }}
  allocatable: pods:{{ .status.allocatable.pods }}, cpu:{{ .status.allocatable.cpu }}, mem:{{ .status.allocatable.memory | quantityToFloat64 | humanizeSI "B" }}, ephemeral-storage:{{index .status.allocatable "ephemeral-storage" | quantityToFloat64 | humanizeSI "B" }}
    {{- end }}
    {{- with .nodeStatsSummary }}
  {{ "kubelet-api/stats/summary" }}:
    node overall: {{ template "node_stats_summary_resources" .node }}
        {{- range .node.systemContainers }}
    {{ .name }}: {{ template "node_stats_summary_resources" . }}
        {{- end }}
    {{- end }}
    {{- template "recent_updates" . }}
    {{- template "events" . }}
    {{- template "owners" . }}
{{- end -}}

{{- define "node_stats_summary_fs" }}
    {{- .usedBytes | float64 | humanizeSI "B" }}/{{ .capacityBytes | float64 | humanizeSI "B" -}}, {{ .availableBytes | float64 | humanizeSI "B" }} still free; {{ "" }}
    {{- .inodesUsed | float64 | humanizeSI "" }}/{{ .inodes | float64 | humanizeSI "" }} inode, {{ .inodesFree | float64 | humanizeSI "" }} inode still free
{{- end -}}

{{- /* -}}
     usage 0.135core/sec(0.01% of available), 1.8GB(%), workingSet 1.4GB, rss 660MB, available 608MB, pageFaults major/minor 0/0,
     TODO: 0.135core/sec(0.01% of available)
     TODO: .cpu.usageCoreNanoSeconds: 71853537319612
    {{- */ -}}
{{- define "node_stats_summary_resources" }}
    {{- with .cpu.usageNanoCores }}cpu {{ . | float64 | divFloat64 1000000000 | printf "%.2g" }}core/sec, {{ end }}
    {{- with .memory -}}
        mem {{ .usageBytes | float64 | humanizeSI "B" -}}
        , workingSet {{ .workingSetBytes | float64 | humanizeSI "B" -}}
        , rss {{ .rssBytes | float64 | humanizeSI "B" -}}
        {{- if .available -}}
        , available {{ .available | float64 | humanizeSI "B" -}}
        {{- end -}}
        , pageFaults/major {{ .pageFaults | float64 | humanizeSI "" }}/{{ .majorPageFaults | float64 | humanizeSI "" }}
    {{- end }}
    {{- with .rlimit -}}
    , processes {{ .curproc | float64  | humanizeSI "" }}/{{ .maxpid | float64  | humanizeSI "" }} (rlimit)
    {{- end }}
    {{- with .network }}
      network: rx/tx {{ .rxBytes | float64 | humanizeSI "B" }}/{{ .txBytes | float64 | humanizeSI "B" }}, rx/tx errors {{ .rxErrors | humanizeSI "" }}/{{ .txErrors | humanizeSI "" }}
    {{- end }}
    {{- if .fs }}
      {{ "rootfs" | bold }}: {{ template "node_stats_summary_fs" .fs }}
        {{- with .runtime.imageFs }}
      {{ "imagefs" | bold }}: {{ template "node_stats_summary_fs" . }}
        {{- end }}
      {{- end }}
{{- end -}}

{{- define "application_details" }}
    {{- $labels := .metadata.labels | default dict }}
    {{- $annotations := .metadata.annotations | default dict }}
    {{- $clusterService := get $labels "kubernetes.io/cluster-service" }}
    {{- $addonManager := get $labels "addonmanager.kubernetes.io/mode" }}
    {{- $managedBy := coalesce (get $labels "app.kubernetes.io/managed-by") $labels.heritage }}
    {{- $createdBy := get $labels "app.kubernetes.io/created-by" }}
    {{- $name := coalesce (get $labels "helm.sh/chart") $labels.chart (get $labels "app.kubernetes.io/name") $labels.app (get $labels "k8s-app") (get $labels "kubernetes.io/name") $labels.ame | default "" }}
    {{- $partOf := get $labels "app.kubernetes.io/part-of" }}
    {{- $component := get $labels "app.kubernetes.io/component" }}
    {{- $instance := coalesce (get $annotations "meta.helm.sh/release-name") $labels.release (get $labels "app.kubernetes.io/instance") }}
    {{- $releaseNamespace := get $annotations "meta.helm.sh/release-namespace" }}
    {{- $namespace := eq $releaseNamespace $.metadata.namespace | ternary "" $releaseNamespace }}
    {{- $version := coalesce (get $labels "app.kubernetes.io/version") $labels.version }}
    {{- if or $clusterService $addonManager $managedBy $createdBy $name $partOf $component $instance $namespace $version }}
  Managed
        {{- with $addonManager }} by {{ "AddonManager" | bold | yellow }} in {{ . | redBoldIf (eq . "Reconcile") }} mode{{ end }}
        {{- with $clusterService }} as a {{ "cluster-service" | yellow | bold }}{{end}}
        {{- with $managedBy }} by {{ . | bold }}{{ end }}
        {{- with $createdBy }} (created by {{ . | bold }}){{ end }}
        {{- with $instance }} with {{ . | bold }} {{ eq $managedBy "Helm" | ternary "release" "instance" }}{{end}}
        {{- with $namespace }} in {{ . | bold | yellow }} namespace (different than the current resource){{end}}
        {{- with $name }} by {{ . | bold }} {{ eq $managedBy "Helm" | ternary "chart" "application" }}{{ end }}
        {{- with $partOf }}{{ if not (contains $partOf $name) }} as part of {{ . | bold }}{{ end }}{{ end }}
        {{- with $component }} within component {{ . | bold }}{{ end }}
        {{- with $version }}{{ if not (contains $version $name) }} using version {{ . | bold }}{{ end }}{{ end }}
    {{- end -}}
{{- end -}}